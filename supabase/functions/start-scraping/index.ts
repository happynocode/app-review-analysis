import { createClient } from 'npm:@supabase/supabase-js@2'

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
  'Access-Control-Allow-Methods': 'POST, OPTIONS',
}

interface StartScrapingRequest {
  reportId: string
  appName: string
  userSearchTerm?: string // ðŸ†• ç”¨æˆ·çš„åŽŸå§‹æœç´¢è¯
  selectedAppName?: string // ðŸ†• ç”¨æˆ·é€‰æ‹©çš„appåç§°
  scrapingSessionId: string
  appInfo?: any
  selectedApps?: any[]
  redditOnly?: boolean // ðŸ†• å‘åŽå…¼å®¹
  enabledPlatforms?: string[] // ðŸ†• å¯ç”¨çš„å¹³å°åˆ—è¡¨
  analysisConfig?: any // ðŸ†• åˆ†æžé…ç½®
  searchContext?: {
    userProvidedName: string
    useUserNameForReddit: boolean
    redditOnlyMode?: boolean
  }
}

Deno.serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders })
  }

  try {
    const supabaseClient = createClient(
      Deno.env.get('SUPABASE_URL') ?? '',
      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
    )

    const { 
      reportId, 
      appName, 
      userSearchTerm,
      selectedAppName,
      scrapingSessionId, 
      appInfo, 
      selectedApps,
      redditOnly,
      enabledPlatforms, // ðŸ†• æŽ¥æ”¶å¯ç”¨çš„å¹³å°
      analysisConfig, // ðŸ†• æŽ¥æ”¶åˆ†æžé…ç½®
      searchContext 
    }: StartScrapingRequest = await req.json()

    if (!reportId || !appName || !scrapingSessionId) {
      return new Response(
        JSON.stringify({ error: 'Missing required parameters' }),
        { 
          status: 400, 
          headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
        }
      )
    }

    console.log(`ðŸš€ Starting scraping for report ${reportId}, app: ${appName}`)
    
    // ðŸ†• ç¡®å®šå¯ç”¨çš„å¹³å°ï¼ˆå‘åŽå…¼å®¹ï¼‰
    const finalEnabledPlatforms = enabledPlatforms || 
      (redditOnly || searchContext?.redditOnlyMode ? ['reddit'] : ['app_store', 'google_play', 'reddit'])
    
    // ðŸ”‘ ç¡®å®š Reddit æœç´¢ä½¿ç”¨çš„åç§°
    const redditSearchName = userSearchTerm || // ðŸ†• ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æœç´¢è¯
      (searchContext?.useUserNameForReddit 
        ? searchContext.userProvidedName 
        : appName)
    
    console.log(`ðŸŽ¯ Enabled platforms: ${finalEnabledPlatforms.join(', ')}`)
    console.log(`ðŸŽ¯ Reddit search will use: "${redditSearchName}" (user search term: "${userSearchTerm || 'not provided'}", selected app: "${selectedAppName || 'not provided'}")`)
    
    // ðŸ†• æ›´æ–°scraping sessionä¸­çš„å¹³å°çŠ¶æ€ï¼ˆå¦‚æžœè¿˜æ²¡æœ‰è®¾ç½®ï¼‰
    const { data: currentSession } = await supabaseClient
      .from('scraping_sessions')
      .select('enabled_platforms')
      .eq('id', scrapingSessionId)
      .single()
    
    if (!currentSession?.enabled_platforms) {
      await supabaseClient
        .from('scraping_sessions')
        .update({
          enabled_platforms: finalEnabledPlatforms,
          analysis_config: analysisConfig || {},
          app_store_scraper_status: finalEnabledPlatforms.includes('app_store') ? 'pending' : 'disabled',
          google_play_scraper_status: finalEnabledPlatforms.includes('google_play') ? 'pending' : 'disabled',
          reddit_scraper_status: finalEnabledPlatforms.includes('reddit') ? 'pending' : 'disabled'
        })
        .eq('id', scrapingSessionId)
    }

    // Update report status to scraping and scraping session to running
    await supabaseClient
      .from('reports')
      .update({ 
        status: 'scraping',
        updated_at: new Date().toISOString()
      })
      .eq('id', reportId)

    await supabaseClient
      .from('scraping_sessions')
      .update({ 
        status: 'running',
        started_at: new Date().toISOString()
      })
      .eq('id', scrapingSessionId)

    // ðŸ†• Start the scraping process with platform configuration
    EdgeRuntime.waitUntil(performScraping(
      reportId, 
      appName, 
      scrapingSessionId, 
      supabaseClient, 
      appInfo, 
      selectedApps,
      redditSearchName,
      finalEnabledPlatforms, // ðŸ†• ä¼ é€’å¯ç”¨çš„å¹³å°
      analysisConfig, // ðŸ†• ä¼ é€’åˆ†æžé…ç½®
      userSearchTerm, // ðŸ†• ä¼ é€’ç”¨æˆ·æœç´¢è¯
      selectedAppName // ðŸ†• ä¼ é€’é€‰ä¸­çš„appåç§°
    ))

    return new Response(
      JSON.stringify({ 
        success: true, 
        message: redditOnly ? 'Reddit-only scraping started' : 'Scraping started',
        reportId,
        scrapingSessionId,
        redditSearchName, // è¿”å›žå®žé™…ç”¨äºŽ Reddit æœç´¢çš„åç§°
        analysisType: redditOnly ? 'reddit_only' : 'comprehensive' // ðŸ†• åˆ†æžç±»åž‹
      }),
      { 
        headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
      }
    )

  } catch (error) {
    console.error('Error in start-scraping:', error)
    return new Response(
      JSON.stringify({ 
        error: 'Internal server error',
        details: error.message 
      }),
      { 
        status: 500, 
        headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
      }
    )
  }
})

async function performScraping(
  reportId: string, 
  appName: string, 
  scrapingSessionId: string, 
  supabaseClient: any,
  appInfo?: any,
  selectedApps?: any[],
  redditSearchName?: string,
  enabledPlatforms?: string[], // ðŸ†• å¯ç”¨çš„å¹³å°
  analysisConfig?: any, // ðŸ†• åˆ†æžé…ç½®
  userSearchTerm?: string, // ðŸ†• ä¼ é€’ç”¨æˆ·æœç´¢è¯
  selectedAppName?: string // ðŸ†• ä¼ é€’é€‰ä¸­çš„appåç§°
) {
  try {
    console.log(`ðŸ“Š Starting scraping process for ${appName}`)
    console.log(`ðŸŽ¯ Reddit search name: "${redditSearchName || appName}"`)
    console.log(`ðŸŽ¯ Enabled platforms: ${enabledPlatforms?.join(', ') || 'all'}`)
    
    const isRedditOnly = enabledPlatforms?.length === 1 && enabledPlatforms[0] === 'reddit'
    const enabledSet = new Set(enabledPlatforms || ['app_store', 'google_play', 'reddit'])
    
    // ðŸ†• æ ¹æ®å¯ç”¨çš„å¹³å°è¿›è¡ŒæŠ“å–
    if (isRedditOnly) {
      console.log(`ðŸŽ¯ Reddit-only mode: Performing Reddit-only scraping`)
      
      // æ›´æ–° Reddit scraper çŠ¶æ€ä¸ºè¿è¡Œä¸­
      await supabaseClient
        .from('scraping_sessions')
        .update({
          reddit_scraper_status: 'running',
          reddit_started_at: new Date().toISOString()
        })
        .eq('id', scrapingSessionId)
      
      const scrapedData = await performRedditOnlyScraping(redditSearchName || appName, scrapingSessionId)
      
      // æ›´æ–° Reddit scraper çŠ¶æ€å’Œæ€»ä½“çŠ¶æ€ä¸ºå®Œæˆ
      const completedAt = new Date().toISOString()
      await supabaseClient
        .from('scraping_sessions')
        .update({
          status: 'completed',
          completed_at: completedAt,
          reddit_scraper_status: scrapedData.totalReviews > 0 ? 'completed' : 'failed',
          reddit_completed_at: completedAt,
          // ç¡®ä¿å…¶ä»–å¹³å°çŠ¶æ€ä¸ºdisabledï¼ˆå¦‚æžœå®ƒä»¬æ²¡æœ‰è¢«å¯ç”¨ï¼‰
          app_store_scraper_status: 'disabled',
          google_play_scraper_status: 'disabled'
        })
        .eq('id', scrapingSessionId)

      console.log(`âœ… Reddit-only scraping completed: Found ${scrapedData.totalReviews} Reddit posts`)
      console.log(`ðŸ”„ Status monitoring will handle completion detection and analysis triggering`)
      return
    }

    // ðŸ”„ æ›´æ–°åŽçš„ç»¼åˆæŠ“å–é€»è¾‘ - çŽ°åœ¨ä¼šä¼ é€’enabledPlatforms
    // Determine scraping strategy based on available app info
    let scrapedData
    if (selectedApps && selectedApps.length > 0) {
      // Multiple apps - scrape each one
      console.log(`Scraping ${selectedApps.length} selected apps...`)
      scrapedData = await scrapeMultipleApps(selectedApps, scrapingSessionId, redditSearchName, enabledPlatforms)
    } else if (appInfo) {
      // Single app with detailed info
      console.log(`Scraping single app with detailed info: ${appInfo.name}`)
      scrapedData = await scrapeSingleAppWithInfo(appInfo, scrapingSessionId, redditSearchName, enabledPlatforms)
    } else {
      // Fallback to general search
      console.log(`Fallback to general search for: ${appName}`)
      scrapedData = await scrapeGeneralSearch(appName, scrapingSessionId, redditSearchName, enabledPlatforms, userSearchTerm, selectedAppName)
    }
    
    console.log(`âœ… Scraping completed: Found ${scrapedData.totalReviews} total reviews`)
    console.log(`- App Store: ${scrapedData.appStore.length}`)
    console.log(`- Google Play: ${scrapedData.googlePlay.length}`)
    console.log(`- Reddit: ${scrapedData.reddit.length} (searched for: "${redditSearchName || appName}")`)
    
    // Update scraping session status and individual platform statuses
    const completedAt = new Date().toISOString()
    const platformUpdates: any = {
      status: 'completed',
      completed_at: completedAt
    }

    // Update individual platform scraper statuses based on results
    // Note: enabledSet is already defined above at line 177

    if (enabledSet.has('app_store')) {
      platformUpdates.app_store_scraper_status = scrapedData.appStore.length > 0 ? 'completed' : 'failed'
      platformUpdates.app_store_completed_at = completedAt
    }

    if (enabledSet.has('google_play')) {
      platformUpdates.google_play_scraper_status = scrapedData.googlePlay.length > 0 ? 'completed' : 'failed'
      platformUpdates.google_play_completed_at = completedAt
    }

    if (enabledSet.has('reddit')) {
      platformUpdates.reddit_scraper_status = scrapedData.reddit.length > 0 ? 'completed' : 'failed'
      platformUpdates.reddit_completed_at = completedAt
    }

    await supabaseClient
      .from('scraping_sessions')
      .update(platformUpdates)
      .eq('id', scrapingSessionId)

    console.log(`âœ… Updated scraping session with platform statuses:`, {
      app_store: platformUpdates.app_store_scraper_status,
      google_play: platformUpdates.google_play_scraper_status,
      reddit: platformUpdates.reddit_scraper_status
    })

    // ä¸å†ç›´æŽ¥è§¦å‘åˆ†æžï¼Œè®©cron-scraping-monitoræ¥å¤„ç†çŠ¶æ€è½¬æ¢å’Œåˆ†æžè§¦å‘
    console.log(`âœ… Scraping data collection completed for report ${reportId}`)
    console.log(`ðŸ”„ Status monitoring will handle completion detection and analysis triggering`)

  } catch (error) {
    console.error(`âŒ Error in scraping process for ${reportId}:`, error)
    
    // Update scraping session with error
    await supabaseClient
      .from('scraping_sessions')
      .update({ 
        status: 'error',
        error_message: error.message,
        completed_at: new Date().toISOString()
      })
      .eq('id', scrapingSessionId)
    
    // Update report status to error
    await supabaseClient
      .from('reports')
      .update({ status: 'error' })
      .eq('id', reportId)
  }
}

// ðŸ†• ä»… Reddit æŠ“å–å‡½æ•°
async function performRedditOnlyScraping(appName: string, scrapingSessionId: string) {
  console.log(`ðŸŽ¯ Performing Reddit-only scraping for: "${appName}"`)
  
  const scrapedData = {
    appStore: [],
    googlePlay: [],
    reddit: [],
    totalReviews: 0
  }

  try {
    // åªè°ƒç”¨ Reddit æŠ“å–
    const redditData = await scrapeRedditForApp(appName, scrapingSessionId)
    scrapedData.reddit = redditData
    scrapedData.totalReviews = redditData.length
    
    console.log(`âœ… Reddit-only scraping completed: ${scrapedData.reddit.length} posts found`)
    
  } catch (error) {
    console.error(`âŒ Error in Reddit-only scraping:`, error)
  }

  return scrapedData
}

// Scrape multiple selected apps
async function scrapeMultipleApps(selectedApps: any[], scrapingSessionId: string, redditSearchName?: string, enabledPlatforms?: string[]) {
  const allData = {
    appStore: [],
    googlePlay: [],
    reddit: [],
    totalReviews: 0
  }

  for (const app of selectedApps) {
    console.log(`Scraping app: ${app.name} (${app.platform})`)
    
    try {
      if (app.platform === 'ios') {
        const appStoreData = await scrapeSpecificIOSApp(app, scrapingSessionId)
        allData.appStore.push(...appStoreData)
      } else if (app.platform === 'android') {
        const googlePlayData = await scrapeSpecificAndroidApp(app, scrapingSessionId)
        allData.googlePlay.push(...googlePlayData)
      }

      // ðŸ”‘ Reddit æœç´¢ä½¿ç”¨ç”¨æˆ·æä¾›çš„åç§°
      const searchName = redditSearchName || app.name
      console.log(`ðŸŽ¯ Reddit search for app ${app.name} using name: "${searchName}"`)
      const redditData = await scrapeRedditForApp(searchName, scrapingSessionId)
      allData.reddit.push(...redditData)

    } catch (error) {
      console.error(`Error scraping app ${app.name}:`, error)
    }
  }

  allData.totalReviews = allData.appStore.length + allData.googlePlay.length + allData.reddit.length
  return allData
}

// Scrape single app (with detailed info)
async function scrapeSingleAppWithInfo(appInfo: any, scrapingSessionId: string, redditSearchName?: string, enabledPlatforms?: string[]) {
  const scrapedData = {
    appStore: [],
    googlePlay: [],
    reddit: [],
    totalReviews: 0
  }

  try {
    if (appInfo.platform === 'ios') {
      scrapedData.appStore = await scrapeSpecificIOSApp(appInfo, scrapingSessionId)
    } else if (appInfo.platform === 'android') {
      scrapedData.googlePlay = await scrapeSpecificAndroidApp(appInfo, scrapingSessionId)
    }

    // ðŸ”‘ Reddit æœç´¢ä½¿ç”¨ç”¨æˆ·æä¾›çš„åç§°
    const searchName = redditSearchName || appInfo.name
    console.log(`ðŸŽ¯ Reddit search for ${appInfo.name} using name: "${searchName}"`)
    scrapedData.reddit = await scrapeRedditForApp(searchName, scrapingSessionId)

  } catch (error) {
    console.error(`Error scraping app ${appInfo.name}:`, error)
  }

  scrapedData.totalReviews = scrapedData.appStore.length + scrapedData.googlePlay.length + scrapedData.reddit.length
  return scrapedData
}

// General search approach (no specific app info)
async function scrapeGeneralSearch(appName: string, scrapingSessionId: string, redditSearchName?: string, enabledPlatforms?: string[], userSearchTerm?: string, selectedAppName?: string) {
  console.log(`ðŸ“± General search approach for: ${appName}`)
  return await startParallelScraping(appName, scrapingSessionId, redditSearchName, enabledPlatforms, userSearchTerm, selectedAppName)
}

// Scrape specific iOS app
async function scrapeSpecificIOSApp(appInfo: any, scrapingSessionId: string) {
  try {
    const response = await fetch(`${Deno.env.get('SUPABASE_URL')}/functions/v1/scrape-app-store`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')}`
      },
      body: JSON.stringify({
        appName: appInfo.name,
        appId: appInfo.id,
        scrapingSessionId
      })
    })

    if (response.ok) {
      const data = await response.json()
      return data.reviews || []
    }
  } catch (error) {
    console.error('Error scraping specific iOS app:', error)
  }
  
  return []
}

// Scrape specific Android app
async function scrapeSpecificAndroidApp(appInfo: any, scrapingSessionId: string) {
  try {
    const response = await fetch(`${Deno.env.get('SUPABASE_URL')}/functions/v1/scrape-google-play`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')}`
      },
      body: JSON.stringify({
        appName: appInfo.name,
        packageName: appInfo.packageId,
        scrapingSessionId
      })
    })

    if (response.ok) {
      const data = await response.json()
      return data.reviews || []
    }
  } catch (error) {
    console.error('Error scraping specific Android app:', error)
  }
  
  return []
}

// Search Reddit for specific app
async function scrapeRedditForApp(appName: string, scrapingSessionId: string, userSearchTerm?: string, selectedAppName?: string) {
  try {
    console.log(`ðŸŽ¯ Calling Reddit scraper with app name: "${appName}"`)
    console.log(`ðŸŽ¯ User search term: "${userSearchTerm || 'not provided'}", Selected app: "${selectedAppName || 'not provided'}"`)

    const response = await fetch(`${Deno.env.get('SUPABASE_URL')}/functions/v1/scrape-reddit`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')}`
      },
      body: JSON.stringify({
        appName: selectedAppName || appName, // ðŸ†• ä½¿ç”¨selectedAppNameä½œä¸ºappåç§°
        userSearchTerm: userSearchTerm, // ðŸ†• ä¼ é€’ç”¨æˆ·æœç´¢è¯
        scrapingSessionId,
        // ç§»é™¤maxPostsé™åˆ¶ï¼Œè®©scrape-redditèŽ·å–æ‰€æœ‰å¯ç”¨æ•°æ®
      })
    })

    if (response.ok) {
      const data = await response.json()
      console.log(`âœ… Reddit scraper returned ${data.posts?.length || 0} posts for "${appName}"`)
      return data.posts || []
    } else {
      console.error(`âŒ Reddit scraper failed: ${response.status}`)
    }
  } catch (error) {
    console.error('Error scraping Reddit for app:', error)
  }
  
  return []
}

function startParallelScraping(appName: string, scrapingSessionId: string, redditSearchName?: string, enabledPlatforms?: string[], userSearchTerm?: string, selectedAppName?: string) {
  const baseUrl = Deno.env.get('SUPABASE_URL')

  const headers = {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')}`
  }

  // ðŸ†• ç¡®å®šå¯ç”¨çš„å¹³å°
  const enabledSet = new Set(enabledPlatforms || ['app_store', 'google_play', 'reddit'])
  const isAppStoreEnabled = enabledSet.has('app_store')
  const isGooglePlayEnabled = enabledSet.has('google_play')
  const isRedditEnabled = enabledSet.has('reddit')

  console.log(`ðŸŽ¯ Parallel scraping setup:`)
  console.log(`   - App Store: ${isAppStoreEnabled ? 'ENABLED' : 'DISABLED'}`)
  console.log(`   - Google Play: ${isGooglePlayEnabled ? 'ENABLED' : 'DISABLED'}`)
  console.log(`   - Reddit: ${isRedditEnabled ? 'ENABLED' : 'DISABLED'}`)
  console.log(`   - App Store/Google Play search: "${appName}"`)
  console.log(`   - Reddit search: "${redditSearchName || appName}"`)

  // ðŸ†• åªä¸ºå¯ç”¨çš„å¹³å°åˆ›å»ºPromise
  const scrapingPromises: any = {}

  // App Store Promise (åªåœ¨å¯ç”¨æ—¶åˆ›å»º)
  if (isAppStoreEnabled) {
    const appStoreRequestBody = JSON.stringify({ appName, scrapingSessionId })
    const appStorePromise = fetch(`${baseUrl}/functions/v1/scrape-app-store`, {
      method: 'POST',
      headers,
      body: appStoreRequestBody
    }).then(async (response) => {
      const result = { platform: 'app_store', success: false, data: null, error: null }
      try {
        if (response.ok) {
          result.data = await response.json()
          result.success = true
          console.log(`App Store scraping completed: ${result.data.reviews?.length || 0} reviews`)
        } else {
          const errorText = await response.text()
          result.error = `HTTP ${response.status}: ${errorText}`
          console.error(`App Store scraping failed: ${result.error}`)
        }
      } catch (error) {
        result.error = error.message
        console.error(`App Store scraping error: ${error.message}`)
      }
      return result
    })
    scrapingPromises.appStore = appStorePromise
  } else {
    // ä¸ºç¦ç”¨çš„å¹³å°åˆ›å»ºä¸€ä¸ªç«‹å³è§£æžçš„Promise
    scrapingPromises.appStore = Promise.resolve({ 
      platform: 'app_store', 
      success: true, 
      data: { reviews: [] }, 
      error: null,
      disabled: true 
    })
  }

  // Google Play Promise (åªåœ¨å¯ç”¨æ—¶åˆ›å»º)
  if (isGooglePlayEnabled) {
    const googlePlayRequestBody = JSON.stringify({ appName, scrapingSessionId })
    const googlePlayPromise = fetch(`${baseUrl}/functions/v1/scrape-google-play`, {
      method: 'POST',
      headers,
      body: googlePlayRequestBody
    }).then(async (response) => {
      const result = { platform: 'google_play', success: false, data: null, error: null }
      try {
        if (response.ok) {
          result.data = await response.json()
          result.success = true
          console.log(`Google Play scraping completed: ${result.data.reviews?.length || 0} reviews`)
        } else {
          const errorText = await response.text()
          result.error = `HTTP ${response.status}: ${errorText}`
          console.error(`Google Play scraping failed: ${result.error}`)
        }
      } catch (error) {
        result.error = error.message
        console.error(`Google Play scraping error: ${error.message}`)
      }
      return result
    })
    scrapingPromises.googlePlay = googlePlayPromise
  } else {
    // ä¸ºç¦ç”¨çš„å¹³å°åˆ›å»ºä¸€ä¸ªç«‹å³è§£æžçš„Promise
    scrapingPromises.googlePlay = Promise.resolve({ 
      platform: 'google_play', 
      success: true, 
      data: { reviews: [] }, 
      error: null,
      disabled: true 
    })
  }

  // Reddit Promise (åªåœ¨å¯ç”¨æ—¶åˆ›å»º)
  if (isRedditEnabled) {
    const redditRequestBody = JSON.stringify({ 
      appName: selectedAppName || redditSearchName || appName, // ðŸ†• ä½¿ç”¨selectedAppNameä½œä¸ºappåç§°
      userSearchTerm: userSearchTerm, // ðŸ†• ä¼ é€’ç”¨æˆ·æœç´¢è¯ 
      scrapingSessionId,
              // ç§»é™¤maxPostsé™åˆ¶ï¼Œè®©scrape-redditèŽ·å–æ‰€æœ‰å¯ç”¨æ•°æ®
    })
    const redditPromise = fetch(`${baseUrl}/functions/v1/scrape-reddit`, {
      method: 'POST',
      headers,
      body: redditRequestBody
    }).then(async (response) => {
      const result = { platform: 'reddit', success: false, data: null, error: null }
      try {
        if (response.ok) {
          result.data = await response.json()
          result.success = true
          console.log(`Reddit scraping completed: ${result.data.posts?.length || 0} posts (searched for: "${redditSearchName || appName}")`)
        } else {
          const errorText = await response.text()
          result.error = `HTTP ${response.status}: ${errorText}`
          console.error(`Reddit scraping failed: ${result.error}`)
        }
      } catch (error) {
        result.error = error.message
        console.error(`Reddit scraping error: ${error.message}`)
      }
      return result
    })
    scrapingPromises.reddit = redditPromise
  } else {
    // ä¸ºç¦ç”¨çš„å¹³å°åˆ›å»ºä¸€ä¸ªç«‹å³è§£æžçš„Promise
    scrapingPromises.reddit = Promise.resolve({ 
      platform: 'reddit', 
      success: true, 
      data: { posts: [] }, 
      error: null,
      disabled: true 
    })
  }

  return waitForScrapingCompletion(scrapingSessionId, scrapingPromises)
}

async function waitForScrapingCompletion(scrapingSessionId: string, scrapingPromises: any) {
  console.log('Waiting for all scraping tasks to complete...')
  
  // Wait for all promises to settle (complete or fail)
  const results = await Promise.allSettled([
    scrapingPromises.appStore,
    scrapingPromises.googlePlay,
    scrapingPromises.reddit
  ])

  const scrapedData = {
    appStore: [],
    googlePlay: [],
    reddit: [],
    totalReviews: 0,
    errors: []
  }

  // Process results
  for (const result of results) {
    if (result.status === 'fulfilled') {
      const platformResult = result.value
      
      if (platformResult.success && platformResult.data) {
        switch (platformResult.platform) {
          case 'app_store':
            scrapedData.appStore = platformResult.data.reviews || []
            break
          case 'google_play':
            scrapedData.googlePlay = platformResult.data.reviews || []
            break
          case 'reddit':
            scrapedData.reddit = platformResult.data.posts || []
            break
        }
      } else if (platformResult.error) {
        scrapedData.errors.push(`${platformResult.platform}: ${platformResult.error}`)
      }
    } else {
      scrapedData.errors.push(`Promise rejected: ${result.reason}`)
    }
  }

  scrapedData.totalReviews = scrapedData.appStore.length + scrapedData.googlePlay.length + scrapedData.reddit.length

  // Log completion status
  console.log('All scraping tasks completed:')
  console.log(`- App Store: ${scrapedData.appStore.length} reviews`)
  console.log(`- Google Play: ${scrapedData.googlePlay.length} reviews`)
  console.log(`- Reddit: ${scrapedData.reddit.length} posts`)
  console.log(`- Total: ${scrapedData.totalReviews} items`)
  
  if (scrapedData.errors.length > 0) {
    console.log('Scraping errors:', scrapedData.errors)
  }

  return scrapedData
}