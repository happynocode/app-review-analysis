import { createClient } from 'npm:@supabase/supabase-js@2'

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
  'Access-Control-Allow-Methods': 'POST, OPTIONS',
}

interface GenerateReportRequest {
  reportId: string
  appName: string
  userSearchTerm?: string // ğŸ†• ç”¨æˆ·çš„åŸå§‹æœç´¢è¯
  selectedAppName?: string // ğŸ†• ç”¨æˆ·é€‰æ‹©çš„appåç§°
  appInfo?: any // Single app detailed information
  selectedApps?: any[] // Multiple apps information
  redditOnly?: boolean // ğŸ†• ä»… Reddit åˆ†ææ ‡è¯†
  enabledPlatforms?: string[] // ğŸ†• ç”¨æˆ·é€‰æ‹©çš„å¹³å°
  analysisConfig?: {
    redditOnly?: boolean
    userProvidedName?: string
    customSettings?: any
  }
}

// Track active report generations to prevent duplicates
const activeReports = new Set<string>()

Deno.serve(async (req) => {
  // Handle CORS preflight requests
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders })
  }

  try {
    const supabaseClient = createClient(
      Deno.env.get('SUPABASE_URL') ?? '',
      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
    )

    const { 
      reportId, 
      appName, 
      userSearchTerm,
      selectedAppName,
      appInfo, 
      selectedApps, 
      redditOnly,
      enabledPlatforms,
      analysisConfig 
    }: GenerateReportRequest = await req.json()

    if (!reportId || !appName) {
      return new Response(
        JSON.stringify({ error: 'Missing reportId or appName' }),
        { 
          status: 400, 
          headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
        }
      )
    }

    // ğŸ†• ç¡®å®šå¯ç”¨çš„å¹³å°
    let finalEnabledPlatforms = enabledPlatforms || ['app_store', 'google_play', 'reddit']
    if (redditOnly) {
      finalEnabledPlatforms = ['reddit']
    }

    // ğŸ†• å‡†å¤‡åˆ†æé…ç½®
    const finalAnalysisConfig = {
      redditOnly: redditOnly || false,
      userProvidedName: appName,
      enabledPlatforms: finalEnabledPlatforms,
      ...analysisConfig
    }

    // Check if this report is already being processed
    if (activeReports.has(reportId)) {
      return new Response(
        JSON.stringify({ 
          error: 'Report generation already in progress',
          reportId 
        }),
        { 
          status: 409, 
          headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
        }
      )
    }

    // Check if report already exists and is completed
    const { data: existingReport } = await supabaseClient
      .from('reports')
      .select('status')
      .eq('id', reportId)
      .single()

    if (existingReport?.status === 'completed') {
      return new Response(
        JSON.stringify({ 
          error: 'Report already completed',
          reportId 
        }),
        { 
          status: 409, 
          headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
        }
      )
    }

    // Add to active reports
    activeReports.add(reportId)

    console.log(`ğŸš€ Starting report generation for ${appName} (${reportId})`)
    console.log(`ğŸ“± User-provided app name: "${userSearchTerm || appName}" (will be used for Reddit search)`)
    
    // ğŸ†• æ£€æŸ¥æ˜¯å¦ä¸ºä»… Reddit åˆ†æ
    if (redditOnly) {
      console.log(`ğŸ¯ Reddit-only analysis mode enabled for "${appName}"`)
    }

    // Update report status to processing (user_search_term, selected_app_name, enabled_platforms already set during INSERT)
    await supabaseClient
      .from('reports')
      .update({
        status: 'processing'
      })
      .eq('id', reportId)

    // ğŸ†• Check if there's already an active scraping session for this report
    const { data: existingSession, error: checkError } = await supabaseClient
      .from('scraping_sessions')
      .select('*')
      .eq('report_id', reportId)
      .in('status', ['pending', 'running'])
      .order('created_at', { ascending: false })
      .limit(1)
      .maybeSingle()

    if (checkError) {
      activeReports.delete(reportId)
      throw new Error(`Failed to check existing sessions: ${checkError.message}`)
    }

    let scrapingSession
    if (existingSession) {
      console.log(`â™»ï¸ Found existing active scraping session ${existingSession.id} for report ${reportId}`)
      scrapingSession = existingSession
    } else {
      // ğŸ†• Create scraping session with platform configuration
      const { data: newSession, error: sessionError } = await supabaseClient
        .from('scraping_sessions')
        .insert({
          report_id: reportId,
          app_name: appName,
          user_search_term: userSearchTerm,
          selected_app_name: selectedAppName,
          status: 'pending',
          enabled_platforms: finalEnabledPlatforms,
          analysis_config: finalAnalysisConfig,
          // ğŸ†• è®¾ç½®æ¯ä¸ªscraperçš„åˆå§‹çŠ¶æ€
          app_store_scraper_status: finalEnabledPlatforms.includes('app_store') ? 'pending' : 'disabled',
          google_play_scraper_status: finalEnabledPlatforms.includes('google_play') ? 'pending' : 'disabled',
          reddit_scraper_status: finalEnabledPlatforms.includes('reddit') ? 'pending' : 'disabled'
        })
        .select()
        .single()

      if (sessionError) {
        activeReports.delete(reportId)
        throw new Error(`Failed to create scraping session: ${sessionError.message}`)
      }

      scrapingSession = newSession
      console.log(`âœ… Created new scraping session ${scrapingSession.id}`)
    }

    // ğŸ†• Start the scraping process with platform configuration
    EdgeRuntime.waitUntil(initiateScrapingProcess(
      reportId,
      userSearchTerm || appName, // ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æœç´¢è¯
      scrapingSession.id,
      userSearchTerm,
      selectedAppName,
      appInfo, 
      selectedApps,
      finalEnabledPlatforms,
      finalAnalysisConfig
    ))

    return new Response(
      JSON.stringify({ 
        success: true, 
        message: redditOnly ? 'Reddit-only analysis started' : 'Report generation started',
        reportId,
        scrapingSessionId: scrapingSession.id,
        userProvidedAppName: appName, // æ˜ç¡®æ˜¾ç¤ºä½¿ç”¨çš„æ˜¯ç”¨æˆ·æä¾›çš„åç§°
        analysisType: redditOnly ? 'reddit_only' : 'comprehensive' // ğŸ†• åˆ†æç±»å‹
      }),
      { 
        headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
      }
    )

  } catch (error) {
    console.error('Error in generate-report:', error)
    return new Response(
      JSON.stringify({ 
        error: 'Internal server error',
        details: error.message 
      }),
      { 
        status: 500, 
        headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
      }
    )
  }
})

async function initiateScrapingProcess(
  reportId: string, 
  userProvidedAppName: string,
  scrapingSessionId: string, 
  userSearchTerm?: string, // ğŸ†• ç”¨æˆ·æœç´¢è¯ï¼Œå¯é€‰
  selectedAppName?: string, // ğŸ†• é€‰ä¸­çš„appåç§°ï¼Œå¯é€‰
  appInfo?: any,
  selectedApps?: any[],
  enabledPlatforms?: string[], // ğŸ†• å¯ç”¨çš„å¹³å°åˆ—è¡¨
  analysisConfig?: any // ğŸ†• åˆ†æé…ç½®
) {
  try {
    console.log(`ğŸ”„ Initiating scraping process for report ${reportId}`)
    console.log(`ğŸ“± User-provided app name: "${userProvidedAppName}"`)
    console.log(`ğŸ¯ Enabled platforms: ${enabledPlatforms?.join(', ') || 'all'}`)
    
    // ğŸ†• åˆ†æé…ç½®æ—¥å¿—
    const isRedditOnly = analysisConfig?.redditOnly || false
    if (isRedditOnly) {
      console.log(`ğŸ¯ Reddit-only mode: Skipping app store scraping`)
    }

    // ğŸ†• Call the start-scraping function with platform configuration
    const scrapingResponse = await fetch(`${Deno.env.get('SUPABASE_URL')}/functions/v1/start-scraping`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        reportId,
        appName: userProvidedAppName,
        userSearchTerm: userSearchTerm,
        selectedAppName: selectedAppName,
        scrapingSessionId,
        appInfo,
        selectedApps,
        enabledPlatforms, // ğŸ†• ä¼ é€’å¯ç”¨çš„å¹³å°
        analysisConfig, // ğŸ†• ä¼ é€’åˆ†æé…ç½®
        // ğŸ†• ä¿æŒå‘åå…¼å®¹
        redditOnly: isRedditOnly,
        searchContext: {
          userProvidedName: userProvidedAppName,
          useUserNameForReddit: true,
          redditOnlyMode: isRedditOnly
        }
      })
    })

    if (!scrapingResponse.ok) {
      const errorData = await scrapingResponse.json()
      throw new Error(`Failed to start scraping: ${errorData.error || scrapingResponse.statusText}`)
    }

    const scrapingResult = await scrapingResponse.json()
    console.log(`âœ… Successfully initiated scraping for report ${reportId}:`, scrapingResult.message)
    console.log(`ğŸ¯ Reddit will search using user-provided name: "${userProvidedAppName}"`)
    
    // ğŸ†• ç¡®è®¤å¹³å°é…ç½®
    if (isRedditOnly) {
      console.log(`âœ… Reddit-only analysis mode confirmed - app store scraping will be skipped`)
    } else {
      console.log(`âœ… Multi-platform scraping started for: ${enabledPlatforms?.join(', ')}`)
    }

  } catch (error) {
    console.error(`âŒ Error initiating scraping for report ${reportId}:`, error)
    
    // Update report status to error
    try {
      const supabaseClient = createClient(
        Deno.env.get('SUPABASE_URL') ?? '',
        Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
      )

      await supabaseClient
        .from('reports')
        .update({ status: 'error' })
        .eq('id', reportId)

      await supabaseClient
        .from('scraping_sessions')
        .update({ 
          status: 'error',
          error_message: error.message,
          completed_at: new Date().toISOString()
        })
        .eq('id', scrapingSessionId)

    } catch (dbError) {
      console.error(`âŒ Error updating database after scraping failure:`, dbError)
    }
  } finally {
    // Remove from active reports
    activeReports.delete(reportId)
  }
}