import { createClient } from 'npm:@supabase/supabase-js@2'

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
  'Access-Control-Allow-Methods': 'POST, OPTIONS',
}

interface GenerateReportRequest {
  reportId: string
  appName: string
  appInfo?: any // Single app detailed information
  selectedApps?: any[] // Multiple apps information
}

// Track active report generations to prevent duplicates
const activeReports = new Set<string>()

Deno.serve(async (req) => {
  // Handle CORS preflight requests
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders })
  }

  try {
    const supabaseClient = createClient(
      Deno.env.get('SUPABASE_URL') ?? '',
      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
    )

    const { reportId, appName, appInfo, selectedApps }: GenerateReportRequest = await req.json()

    if (!reportId || !appName) {
      return new Response(
        JSON.stringify({ error: 'Missing reportId or appName' }),
        { 
          status: 400, 
          headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
        }
      )
    }

    // Check if this report is already being processed
    if (activeReports.has(reportId)) {
      return new Response(
        JSON.stringify({ 
          error: 'Report generation already in progress',
          reportId 
        }),
        { 
          status: 409, 
          headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
        }
      )
    }

    // Check if report already exists and is completed
    const { data: existingReport } = await supabaseClient
      .from('reports')
      .select('status')
      .eq('id', reportId)
      .single()

    if (existingReport?.status === 'completed') {
      return new Response(
        JSON.stringify({ 
          error: 'Report already completed',
          reportId 
        }),
        { 
          status: 409, 
          headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
        }
      )
    }

    // Add to active reports
    activeReports.add(reportId)

    console.log(`ğŸš€ Starting report generation for ${appName} (${reportId})`)
    console.log(`ğŸ“± User-provided app name: "${appName}" (will be used for Reddit search)`)

    // Update report status to processing
    await supabaseClient
      .from('reports')
      .update({ status: 'processing' })
      .eq('id', reportId)

    // Create scraping session with app info
    const { data: scrapingSession, error: sessionError } = await supabaseClient
      .from('scraping_sessions')
      .insert({
        report_id: reportId,
        app_name: appName,
        status: 'pending'
      })
      .select()
      .single()

    if (sessionError) {
      activeReports.delete(reportId)
      throw new Error(`Failed to create scraping session: ${sessionError.message}`)
    }

    console.log(`âœ… Created scraping session ${scrapingSession.id}`)

    // Start the scraping process by calling the start-scraping function
    // ğŸ”‘ å…³é”®ä¿®å¤ï¼šä¼ é€’åŸå§‹ç”¨æˆ·è¾“å…¥çš„åº”ç”¨åç§°
    EdgeRuntime.waitUntil(initiateScrapingProcess(
      reportId, 
      appName, // ğŸ¯ è¿™æ˜¯ç”¨æˆ·å¡«å†™çš„åŸå§‹åç§°ï¼Œç”¨äº Reddit æœç´¢
      scrapingSession.id, 
      appInfo, 
      selectedApps
    ))

    return new Response(
      JSON.stringify({ 
        success: true, 
        message: 'Report generation started',
        reportId,
        scrapingSessionId: scrapingSession.id,
        userProvidedAppName: appName // æ˜ç¡®æ˜¾ç¤ºä½¿ç”¨çš„æ˜¯ç”¨æˆ·æä¾›çš„åç§°
      }),
      { 
        headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
      }
    )

  } catch (error) {
    console.error('Error in generate-report:', error)
    return new Response(
      JSON.stringify({ 
        error: 'Internal server error',
        details: error.message 
      }),
      { 
        status: 500, 
        headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
      }
    )
  }
})

async function initiateScrapingProcess(
  reportId: string, 
  userProvidedAppName: string, // ğŸ”‘ æ˜ç¡®æ ‡è¯†è¿™æ˜¯ç”¨æˆ·æä¾›çš„åç§°
  scrapingSessionId: string, 
  appInfo?: any,
  selectedApps?: any[]
) {
  try {
    console.log(`ğŸ”„ Initiating scraping process for report ${reportId}`)
    console.log(`ğŸ“± User-provided app name for Reddit search: "${userProvidedAppName}"`)

    // Call the start-scraping function
    // ğŸ¯ ä¼ é€’ç”¨æˆ·æä¾›çš„åº”ç”¨åç§°ï¼Œç¡®ä¿ Reddit æœç´¢ä½¿ç”¨æ­£ç¡®çš„åç§°
    const scrapingResponse = await fetch(`${Deno.env.get('SUPABASE_URL')}/functions/v1/start-scraping`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${Deno.env.get('SUPABASE_ANON_KEY')}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        reportId,
        appName: userProvidedAppName, // ğŸ”‘ ç¡®ä¿ä¼ é€’ç”¨æˆ·åŸå§‹è¾“å…¥
        scrapingSessionId,
        appInfo,
        selectedApps,
        // ğŸ†• æ·»åŠ é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        searchContext: {
          userProvidedName: userProvidedAppName,
          useUserNameForReddit: true // æ˜ç¡®æŒ‡ç¤º Reddit æœç´¢ä½¿ç”¨ç”¨æˆ·åç§°
        }
      })
    })

    if (!scrapingResponse.ok) {
      const errorData = await scrapingResponse.json()
      throw new Error(`Failed to start scraping: ${errorData.error || scrapingResponse.statusText}`)
    }

    const scrapingResult = await scrapingResponse.json()
    console.log(`âœ… Successfully initiated scraping for report ${reportId}:`, scrapingResult.message)
    console.log(`ğŸ¯ Reddit will search using user-provided name: "${userProvidedAppName}"`)

  } catch (error) {
    console.error(`âŒ Error initiating scraping for report ${reportId}:`, error)
    
    // Update report status to error
    try {
      const supabaseClient = createClient(
        Deno.env.get('SUPABASE_URL') ?? '',
        Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
      )

      await supabaseClient
        .from('reports')
        .update({ status: 'error' })
        .eq('id', reportId)

      await supabaseClient
        .from('scraping_sessions')
        .update({ 
          status: 'error',
          error_message: error.message,
          completed_at: new Date().toISOString()
        })
        .eq('id', scrapingSessionId)

    } catch (dbError) {
      console.error(`âŒ Error updating database after scraping failure:`, dbError)
    }
  } finally {
    // Remove from active reports
    activeReports.delete(reportId)
  }
}