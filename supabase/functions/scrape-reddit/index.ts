import { createClient } from 'npm:@supabase/supabase-js@2'

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'x-client-info, apikey, content-type',
  'Access-Control-Allow-Methods': 'POST, OPTIONS',
}

// Reddit API configuration
const REDDIT_CLIENT_ID = Deno.env.get('REDDIT_CLIENT_ID')
const REDDIT_CLIENT_SECRET = Deno.env.get('REDDIT_CLIENT_SECRET')
const REDDIT_USER_AGENT = Deno.env.get('REDDIT_USER_AGENT') || 'FeedbackLens/1.0 by YourUsername'

interface ScrapeRequest {
  appName: string // ç”¨æˆ·é€‰æ‹©çš„åº”ç”¨åç§°ï¼ˆä»åº”ç”¨åˆ—è¡¨ä¸­é€‰æ‹©çš„å®Œæ•´åç§°ï¼‰
  userSearchTerm?: string // ğŸ†• ç”¨æˆ·åœ¨æœç´¢æ¡†è¾“å…¥çš„åŸå§‹å…³é”®è¯
  scrapingSessionId?: string
  maxPosts?: number // ç§»é™¤é»˜è®¤é™åˆ¶
}

interface RedditPost {
  text: string
  title: string
  score: number
  date: string
  subreddit: string
  url: string
  author: string
  searchTerm?: string
  upvoteRatio?: number
  commentCount?: number
  postId?: string
  gilded?: number
  isStickied?: boolean
}

// æœç´¢ä»»åŠ¡æ¥å£
interface SearchTask {
  term: string
  subreddit?: string
  limit: number
  priority: number // 1=highest, 3=lowest
  type: 'global' | 'subreddit' | 'app-specific' | 'pattern'
}

class RedditAPIClient {
  private accessToken: string | null = null
  private tokenExpiry: number = 0
  private rateLimitDelay = 1000 // 1 second between requests

  constructor() {
    if (!REDDIT_CLIENT_ID || !REDDIT_CLIENT_SECRET) {
      console.warn('âš ï¸ Reddit API credentials not configured. Reddit scraping will be limited.')
    }
  }

  private async delay(ms: number): Promise<void> {
    await new Promise(resolve => setTimeout(resolve, ms))
  }

  // è·å– Reddit API è®¿é—®ä»¤ç‰Œ
  async getAccessToken(): Promise<string | null> {
    if (!REDDIT_CLIENT_ID || !REDDIT_CLIENT_SECRET) {
      return null
    }

    // æ£€æŸ¥ç°æœ‰ä»¤ç‰Œæ˜¯å¦ä»ç„¶æœ‰æ•ˆ
    if (this.accessToken && Date.now() < this.tokenExpiry) {
      return this.accessToken
    }

    try {
      console.log('ğŸ” Obtaining Reddit API access token...')
      
      const credentials = btoa(`${REDDIT_CLIENT_ID}:${REDDIT_CLIENT_SECRET}`)
      
      const response = await fetch('https://www.reddit.com/api/v1/access_token', {
        method: 'POST',
        headers: {
          'Authorization': `Basic ${credentials}`,
          'Content-Type': 'application/x-www-form-urlencoded',
          'User-Agent': REDDIT_USER_AGENT
        },
        body: 'grant_type=client_credentials'
      })

      if (!response.ok) {
        const errorText = await response.text()
        throw new Error(`Token request failed: ${response.status} - ${errorText}`)
      }

      const data = await response.json()
      
      if (data.access_token) {
        this.accessToken = data.access_token
        this.tokenExpiry = Date.now() + (data.expires_in * 1000) - 60000 // å‡å»1åˆ†é’Ÿä½œä¸ºç¼“å†²
        console.log(`âœ… Reddit API token obtained, expires in ${data.expires_in} seconds`)
        return this.accessToken
      } else {
        throw new Error('No access token in response')
      }

    } catch (error) {
      console.error('âŒ Failed to obtain Reddit API token:', error.message)
      return null
    }
  }

  // ä½¿ç”¨ Reddit API æœç´¢
  async searchWithAPI(query: string, subreddit?: string, limit: number = 100): Promise<RedditPost[]> {
    const token = await this.getAccessToken()
    if (!token) {
      console.log('âš ï¸ No Reddit API token available, skipping API search')
      return []
    }

    try {
      let searchUrl = 'https://oauth.reddit.com/search'
      const params = new URLSearchParams({
        q: query,
        sort: 'relevance',
        t: 'year', // ğŸ†• é™åˆ¶åœ¨ä¸€å¹´å†…çš„å¸–å­
        limit: limit.toString(),
        type: 'link'
      })

      if (subreddit) {
        params.append('restrict_sr', 'true')
        searchUrl = `https://oauth.reddit.com/r/${subreddit}/search`
      }

      const fullUrl = `${searchUrl}?${params.toString()}`
      
      console.log(`ğŸ” Reddit API search: ${subreddit ? `r/${subreddit}` : 'all'} for "${query}"`)
      
      const response = await fetch(fullUrl, {
        headers: {
          'Authorization': `Bearer ${token}`,
          'User-Agent': REDDIT_USER_AGENT
        }
      })

      if (!response.ok) {
        if (response.status === 401) {
          // Token expired, clear it
          this.accessToken = null
          this.tokenExpiry = 0
        }
        throw new Error(`API search failed: ${response.status}`)
      }

      const data = await response.json()
      
      if (data?.data?.children) {
        const posts = this.parseRedditAPIData(data.data.children, query)
        console.log(`âœ… Reddit API found ${posts.length} posts`)
        return posts
      }

      return []

    } catch (error) {
      console.error(`âŒ Reddit API search error for "${query}":`, error.message)
      return []
    }
  }

  // è§£æ Reddit API æ•°æ®
  private parseRedditAPIData(children: any[], searchTerm: string): RedditPost[] {
    const posts: RedditPost[] = []

    for (const child of children) {
      try {
        const post = child.data
        if (!post) continue

        // è¿‡æ»¤æ‰è¢«åˆ é™¤æˆ–ç§»é™¤çš„å¸–å­
        if (post.removed_by_category || post.banned_by || 
            post.title === '[removed]' || post.title === '[deleted]') {
          continue
        }

        const title = post.title || ''
        const selftext = post.selftext || ''
        const content = selftext || title

        // æœ€å°å†…å®¹é•¿åº¦æ£€æŸ¥
        if (content.length < 20) continue

        posts.push({
          text: content,
          title: title,
          score: post.score || 0,
          date: new Date(post.created_utc * 1000).toISOString().split('T')[0],
          subreddit: post.subreddit || 'unknown',
          url: `https://reddit.com${post.permalink}`,
          author: post.author || 'Anonymous',
          searchTerm: searchTerm,
          upvoteRatio: post.upvote_ratio || 0,
          commentCount: post.num_comments || 0,
          postId: post.id || '',
          gilded: post.gilded || 0,
          isStickied: post.stickied || false
        })

      } catch (error) {
        console.error('Error parsing Reddit API post:', error)
        continue
      }
    }

    return posts
  }
}

// å¹¶è¡Œæ‰¹å¤„ç†å™¨
class BatchProcessor {
  private maxConcurrency: number
  private batchDelay: number
  private requestTimeout: number

  constructor(maxConcurrency = 8, batchDelay = 300, requestTimeout = 10000) {
    this.maxConcurrency = maxConcurrency
    this.batchDelay = batchDelay
    this.requestTimeout = requestTimeout
  }

  // å¹¶è¡Œæ‰§è¡Œæœç´¢ä»»åŠ¡
  async processBatches<T>(
    tasks: Array<() => Promise<T>>,
    onProgress?: (completed: number, total: number) => void
  ): Promise<T[]> {
    const results: T[] = []
    let completed = 0

    for (let i = 0; i < tasks.length; i += this.maxConcurrency) {
      const batch = tasks.slice(i, i + this.maxConcurrency)
      
      console.log(`ğŸ”„ Processing batch ${Math.floor(i / this.maxConcurrency) + 1}/${Math.ceil(tasks.length / this.maxConcurrency)} (${batch.length} tasks)`)
      
      try {
        // å¹¶è¡Œæ‰§è¡Œå½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡
        const batchResults = await Promise.allSettled(
          batch.map(task => this.withTimeout(task(), this.requestTimeout))
        )

        // æ”¶é›†æˆåŠŸçš„ç»“æœ
        for (const result of batchResults) {
          if (result.status === 'fulfilled' && result.value) {
            results.push(result.value)
          } else if (result.status === 'rejected') {
            console.warn(`âš ï¸ Task failed:`, result.reason?.message || 'Unknown error')
          }
        }

        completed += batch.length
        onProgress?.(completed, tasks.length)

        // æ‰¹æ¬¡é—´å»¶è¿Ÿ
        if (i + this.maxConcurrency < tasks.length) {
          await this.delay(this.batchDelay)
        }

      } catch (error) {
        console.error(`âŒ Batch processing error:`, error)
      }
    }

    return results
  }

  // è¶…æ—¶åŒ…è£…å™¨
  private async withTimeout<T>(promise: Promise<T>, timeoutMs: number): Promise<T> {
    const timeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(() => reject(new Error(`Timeout after ${timeoutMs}ms`)), timeoutMs)
    })
    
    return Promise.race([promise, timeoutPromise])
  }

  private async delay(ms: number): Promise<void> {
    await new Promise(resolve => setTimeout(resolve, ms))
  }
}

class OptimizedRedditScraper {
  private apiClient: RedditAPIClient
  private batchProcessor: BatchProcessor
  private seenPostIds: Set<string>

  constructor() {
    this.apiClient = new RedditAPIClient()
    this.batchProcessor = new BatchProcessor(8, 300, 12000) // 8å¹¶å‘ï¼Œ300msæ‰¹æ¬¡å»¶è¿Ÿï¼Œ12ç§’è¶…æ—¶
    this.seenPostIds = new Set()
  }

  // ğŸ†• ç®€åŒ–çš„å…³é”®è¯ç”Ÿæˆï¼šåªä½¿ç”¨æ ¸å¿ƒè¯æ±‡+ç‰¹å®šåç¼€
  private generateOptimizedSearchTerms(userSearchTerm?: string, appName?: string): string[] {
    const searchTerms = new Set<string>()
    
    console.log(`ğŸ”§ Generating simplified search terms - User: "${userSearchTerm || 'none'}", App: "${appName || 'none'}"`)
    
    // ç¡®å®šæ ¸å¿ƒæœç´¢è¯ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æœç´¢è¯ï¼Œå…¶æ¬¡æ˜¯åº”ç”¨åçš„æ ¸å¿ƒå…³é”®è¯
    let coreSearchTerm = ''
    
    if (userSearchTerm && userSearchTerm.trim().length > 0) {
      coreSearchTerm = userSearchTerm.trim().toLowerCase()
      console.log(`ğŸ¯ Using user search term as core: "${coreSearchTerm}"`)
    } else if (appName && appName.trim().length > 0) {
      // ä»åº”ç”¨åæå–ç¬¬ä¸€ä¸ªæ ¸å¿ƒå…³é”®è¯
      const appKeywords = this.extractSimpleAppKeywords(appName)
      if (appKeywords.length > 0) {
        coreSearchTerm = appKeywords[0]
        console.log(`ğŸ“± Using app keyword as core: "${coreSearchTerm}"`)
      }
    }
    
    // å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ ¸å¿ƒæœç´¢è¯ï¼Œè¿”å›ç©ºæ•°ç»„
    if (!coreSearchTerm || coreSearchTerm.length < 2) {
      console.log('âš ï¸ No valid core search term found')
      return []
    }
    
    // å®šä¹‰æœç´¢åç¼€ - æ‰©å±•ç‰ˆæœ¬
    const searchSuffixes = [
      // è¯„ä»·ç›¸å…³
      'review',
      'reviews',
      'rating',
      'ratings',
      'opinion',
      'opinions',
      'feedback',
      'thoughts',
      
      // å¹³å°ç›¸å…³
      'app',
      'application',
      'ios',
      'android',
      'mobile',
      'download',
      
      // é—®é¢˜ç›¸å…³
      'issue',
      'issues',
      'problem',
      'problems',
      'bug',
      'bugs',
      'error',
      'errors',
      'crash',
      'crashes',
      'glitch',
      'glitches',
      
      // ä½“éªŒç›¸å…³
      'experience',
      'experiences',
      'using',
      'tried',
      'testing',
      'working',
      'not working',
      'broken',
      'fixed',
      
      // æ¯”è¾ƒç›¸å…³
      'vs',
      'versus',
      'compared to',
      'alternative',
      'alternatives',
      'better than',
      'worse than',
      'similar to',
      
      // æ¨èç›¸å…³
      'recommend',
      'recommendation',
      'worth it',
      'good',
      'bad',
      'terrible',
      'awesome',
      'amazing',
      'disappointing',
      
      // åŠŸèƒ½ç›¸å…³
      'update',
      'updates',
      'new version',
      'latest version',
      'feature',
      'features',
      'settings',
      'setup',
      
      // ä½¿ç”¨ç›¸å…³
      'how to use',
      'tutorial',
      'guide',
      'tips',
      'tricks',
      'help'
    ]
    
    // ç”Ÿæˆæ ¸å¿ƒè¯æ±‡+åç¼€çš„ç»„åˆ
    for (const suffix of searchSuffixes) {
      searchTerms.add(`${coreSearchTerm} ${suffix}`)
    }
    
    // ä¹Ÿæ·»åŠ å•ç‹¬çš„æ ¸å¿ƒè¯æ±‡
    searchTerms.add(coreSearchTerm)
    searchTerms.add(`"${coreSearchTerm}"`) // ç²¾ç¡®åŒ¹é…
    
    // è½¬æ¢ä¸ºæ•°ç»„å¹¶è¿‡æ»¤
    const finalTerms = Array.from(searchTerms)
      .filter(term => {
        // åŸºæœ¬éªŒè¯
        if (!term || term.length < 3 || term.length > 40) return false
        if (term.includes('undefined') || term.includes('null')) return false
        
        // é¿å…åŒ…å«HTMLå®ä½“æˆ–ç‰¹æ®Šç¼–ç 
        if (term.includes('&amp;') || term.includes('&quot;')) return false
        
        return true
      })
      .slice(0, 25) // å¢åŠ åˆ°æœ€å¤š25ä¸ªæœç´¢è¯ä»¥è·å¾—æ›´å¤šè¦†ç›–

    console.log(`ğŸ“ Generated ${finalTerms.length} expanded search terms:`, finalTerms)
    return finalTerms
  }

  // ğŸ†• ä»åº”ç”¨åæå–ç®€å•å…³é”®è¯çš„è¾…åŠ©æ–¹æ³•
  private extractSimpleAppKeywords(appName: string): string[] {
    // æ¸…ç†åº”ç”¨åï¼šå»æ‰ç‰¹æ®Šå­—ç¬¦å’Œå¸¸è§åç¼€
    let cleanAppName = appName
      .toLowerCase()
      .replace(/[^\w\s]/g, ' ') // ç§»é™¤ç‰¹æ®Šå­—ç¬¦
      .replace(/\s+-\s+/g, ' ') // ç§»é™¤ " - "
      .replace(/\b(app|application|mobile|inc|llc|ltd|corp|company|&amp|amp)\b/gi, ' ') // ç§»é™¤å¸¸è§åç¼€
      .replace(/\s+/g, ' ') // åˆå¹¶å¤šä¸ªç©ºæ ¼
      .trim()
    
    console.log(`ğŸ”§ Cleaned app name: "${appName}" -> "${cleanAppName}"`)
    
    // æå–æœ‰æ„ä¹‰çš„è¯æ±‡
    const keywords = cleanAppName.split(/\s+/)
      .filter(word => word.length > 2)
      .filter(word => !['the', 'and', 'for', 'with', 'app', 'mobile', 'application', 'drive', 'deliver', 'driver'].includes(word.toLowerCase()))
      .slice(0, 3) // åªå–å‰3ä¸ªæœ€é‡è¦çš„è¯
    
    console.log(`ğŸ¯ Extracted app keywords:`, keywords)
    return keywords
  }

  // è·å–é‡ç‚¹ subredditsï¼ˆå¢å¼ºç‰ˆï¼‰
  private getTargetSubreddits(): string[] {
    return [
      // åº”ç”¨å’Œè½¯ä»¶ç›¸å…³ (é«˜ä¼˜å…ˆçº§)
      'apps', 'androidapps', 'iosapps', 'AppReviews', 'software', 'SoftwareRecommendations',
      'AppHookup', 'AppleWatch', 'iPhone', 'iPad', 'Android', 'GooglePlay', 'AppStore',
      
      // æŠ€æœ¯å’Œå¹³å°ç›¸å…³
      'technology', 'tech', 'TechSupport', 'TechReviews', 'gadgets', 'apple', 'google',
      'microsoft', 'opensource', 'Programming', 'webdev', 'MacApps', 'WindowsApps',
      
      // ç”Ÿäº§åŠ›å’Œå·¥ä½œç›¸å…³
      'productivity', 'ProductivityApps', 'WorkflowApps', 'studytips', 'LifeProTips',
      'GetStudying', 'organization', 'selfimprovement',
      
      // ç”¨æˆ·è®¨è®ºå’Œæ¨è
      'AskReddit', 'NoStupidQuestions', 'tipofmytongue', 'HelpMeFind', 'findareddit',
      'reviews', 'BuyItForLife', 'YouShouldKnow', 'LifeHacks',
      
      // æ¸¸æˆå’Œå¨±ä¹ç›¸å…³
      'gaming', 'AndroidGaming', 'iosGaming', 'GameReviews', 'MobileGaming',
      'indiegames', 'GameDeals', 'Steam',
      
      // ç¤¾äº¤å’Œé€šè®¯ç›¸å…³
      'socialmedia', 'privacy', 'security', 'Telegram', 'WhatsApp', 'Signal',
      'Instagram', 'Twitter', 'Facebook', 'TikTok', 'YouTube',
      
      // é‡‘èå’Œå•†åŠ¡ç›¸å…³
      'personalfinance', 'investing', 'CryptoCurrency', 'Entrepreneur', 'smallbusiness',
      'Banking', 'FinTech', 'ecommerce', 'startups',
      
      // è®¾è®¡å’Œåˆ›æ„
      'Design', 'GraphicDesign', 'UserExperience', 'UI_Design', 'web_design',
      'photography', 'AdobeIllustrator', 'photoshop',
      
      // å¥åº·å’Œç”Ÿæ´»æ–¹å¼
      'fitness', 'nutrition', 'loseit', 'getmotivated', 'selfcare',
      'meditation', 'sleep', 'running', 'bodyweightfitness'
    ]
  }

  // ğŸ†• ç®€åŒ–çš„åº”ç”¨ç‰¹å®šsubredditç”Ÿæˆ
  private generateAppSpecificSubreddits(userSearchTerm?: string, appName?: string): string[] {
    const appSubreddits: string[] = []
    
    console.log(`ğŸ¯ Generating simplified app-specific subreddits - User: "${userSearchTerm || 'none'}", App: "${appName || 'none'}"`)
    
    // ç¡®å®šæ ¸å¿ƒæœç´¢è¯ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æœç´¢è¯ï¼Œå…¶æ¬¡æ˜¯åº”ç”¨åçš„æ ¸å¿ƒå…³é”®è¯
    let coreSearchTerm = ''
    
    if (userSearchTerm && userSearchTerm.trim().length > 0) {
      coreSearchTerm = userSearchTerm.trim().toLowerCase()
        .replace(/[^\w\s]/g, '')
        .replace(/\s+/g, '')
      console.log(`ğŸ¯ Using user search term for subreddits: "${coreSearchTerm}"`)
    } else if (appName && appName.trim().length > 0) {
      const appKeywords = this.extractSimpleAppKeywords(appName)
      if (appKeywords.length > 0) {
        coreSearchTerm = appKeywords[0].replace(/[^\w\s]/g, '').replace(/\s+/g, '')
        console.log(`ğŸ“± Using app keyword for subreddits: "${coreSearchTerm}"`)
      }
    }
    
    // å¦‚æœæœ‰æœ‰æ•ˆçš„æ ¸å¿ƒæœç´¢è¯ï¼Œç”Ÿæˆå¯èƒ½çš„subredditåç§°
    if (coreSearchTerm && coreSearchTerm.length >= 3 && coreSearchTerm.length <= 15) {
      appSubreddits.push(coreSearchTerm)
      
      // åªå¯¹çŸ¥åå“ç‰Œæ·»åŠ æœ€ç›¸å…³çš„åç¼€
      const knownBrands = ['uber', 'lyft', 'doordash', 'grubhub', 'postmates', 'spotify', 'netflix', 'amazon', 'google', 'apple', 'microsoft']
      if (knownBrands.includes(coreSearchTerm)) {
        // åªæ·»åŠ æœ€å¸¸è§å’Œç›¸å…³çš„åç¼€
        if (['uber', 'lyft', 'doordash', 'grubhub', 'postmates'].includes(coreSearchTerm)) {
          appSubreddits.push(`${coreSearchTerm}driver`)
          appSubreddits.push(`${coreSearchTerm}drivers`)
        }
      }
    }
    
    // è¿‡æ»¤å¹¶è¿”å›åˆç†çš„subredditåç§°
    const uniqueSubreddits = [...new Set(appSubreddits)]
      .filter(sub => {
        // åŸºæœ¬æ ¼å¼æ£€æŸ¥
        if (sub.length < 3 || sub.length > 21) return false
        if (!/^[a-z0-9]+$/i.test(sub)) return false
        return true
      })
      .slice(0, 3) // åªä¿ç•™æœ€å¤š3ä¸ªç›¸å…³çš„subreddit

    console.log(`ğŸ¯ Generated ${uniqueSubreddits.length} simplified app-specific subreddits:`, uniqueSubreddits)
    return uniqueSubreddits
  }

  // ğŸš€ ä¼˜åŒ–çš„ä¸»æœç´¢æ–¹æ³•ï¼šå¹¶è¡Œæ‰¹å¤„ç†ç­–ç•¥
  async scrapeReddit(userSearchTerm?: string, appName?: string, maxPosts?: number): Promise<RedditPost[]> {
    const allPosts: RedditPost[] = []
    
    console.log(`\nğŸš€ === OPTIMIZED PARALLEL REDDIT SCRAPER ===`)
    console.log(`ğŸ‘¤ User search term: "${userSearchTerm || 'not provided'}"`)
    console.log(`ğŸ“± App name: "${appName || 'not provided'}"`)
    console.log(`ğŸ”‘ Reddit API: ${REDDIT_CLIENT_ID ? 'Configured' : 'Not configured'}`)
    console.log(`ğŸ“Š Target max posts: ${maxPosts || 'unlimited - scraping all posts'}`)
    console.log(`âš¡ Parallel processing: 8 concurrent requests`)
    console.log(`â° Start Time: ${new Date().toISOString()}`)

    // æ£€æŸ¥APIå¯ç”¨æ€§
    if (!REDDIT_CLIENT_ID || !REDDIT_CLIENT_SECRET) {
      console.error('âŒ Reddit API credentials not configured. Cannot proceed with scraping.')
      return []
    }

    // æ¸…ç©ºå·²è§å¸–å­IDé›†åˆ
    this.seenPostIds.clear()

    const searchTerms = this.generateOptimizedSearchTerms(userSearchTerm, appName)
    const generalSubreddits = this.getTargetSubreddits()
    const appSpecificSubreddits = this.generateAppSpecificSubreddits(userSearchTerm, appName)

    console.log(`ğŸ¯ Search terms generated: ${searchTerms.length}`)
    console.log(`ğŸ“¡ General subreddits: ${generalSubreddits.length}`)
    console.log(`ğŸª App-specific subreddits: ${appSpecificSubreddits.length}`)

    try {
      // åˆ›å»ºæœç´¢ä»»åŠ¡é˜Ÿåˆ—
      const searchTasks: SearchTask[] = []

      // 1. å…¨å±€æœç´¢ä»»åŠ¡ (æœ€é«˜ä¼˜å…ˆçº§)
      for (const term of searchTerms.slice(0, 15)) {
        searchTasks.push({
          term,
          limit: 100,
          priority: 1,
          type: 'global'
        })
      }

      // 2. çƒ­é—¨subredditæœç´¢ä»»åŠ¡
      const topSubreddits = generalSubreddits.slice(0, 12) // é€‰æ‹©æœ€é‡è¦çš„12ä¸ª
      for (const subreddit of topSubreddits) {
        for (const term of searchTerms.slice(0, 6)) { // æ¯ä¸ªsubredditåªæœç´¢6ä¸ªæœ€é‡è¦çš„å…³é”®è¯
          searchTasks.push({
            term,
            subreddit,
            limit: 50,
            priority: 2,
            type: 'subreddit'
          })
        }
      }

      // 3. åº”ç”¨ç‰¹å®šsubredditæœç´¢ä»»åŠ¡
      for (const appSubreddit of appSpecificSubreddits.slice(0, 8)) { // é™åˆ¶åˆ°8ä¸ªåº”ç”¨ç‰¹å®šsubreddit
        for (const term of searchTerms.slice(0, 4)) { // æ¯ä¸ªåº”ç”¨subredditæœç´¢4ä¸ªå…³é”®è¯
          searchTasks.push({
            term,
            subreddit: appSubreddit,
            limit: 30,
            priority: 2,
            type: 'app-specific'
          })
        }
      }

      // 4. é«˜ä»·å€¼æ¨¡å¼æœç´¢ä»»åŠ¡
      if (userSearchTerm || appName) {
        const coreSearchTerm = userSearchTerm?.trim().toLowerCase() || 
          this.extractSimpleAppKeywords(appName || '')[0]
        
        if (coreSearchTerm && coreSearchTerm.length > 2) {
          const highValueSuffixes = ['vs', 'alternative', 'better than', 'review', 'opinion']
          for (const suffix of highValueSuffixes) {
            searchTasks.push({
              term: `${coreSearchTerm} ${suffix}`,
              limit: 40,
              priority: 1,
              type: 'pattern'
            })
          }
        }
      }

      // æŒ‰ä¼˜å…ˆçº§æ’åºä»»åŠ¡
      searchTasks.sort((a, b) => a.priority - b.priority)

      console.log(`ğŸ“‹ Total search tasks created: ${searchTasks.length}`)
      console.log(`ğŸ”¥ High priority tasks: ${searchTasks.filter(t => t.priority === 1).length}`)
      console.log(`ğŸ“Š Medium priority tasks: ${searchTasks.filter(t => t.priority === 2).length}`)

      // åˆ›å»ºæœç´¢å‡½æ•°
      const searchFunctions = searchTasks.map(task => async () => {
        try {
          const posts = await this.apiClient.searchWithAPI(task.term, task.subreddit, task.limit)
          
          // å®æ—¶å»é‡
          const newPosts = posts.filter(post => {
            const postKey = post.postId || `${post.title}_${post.author}_${post.date}`
            if (this.seenPostIds.has(postKey)) {
              return false
            }
            this.seenPostIds.add(postKey)
            return true
          })

          console.log(`âœ… ${task.type} "${task.term}"${task.subreddit ? ` in r/${task.subreddit}` : ''}: ${newPosts.length} new posts`)
          return newPosts
        } catch (error) {
          console.warn(`âš ï¸ Search failed for "${task.term}": ${error.message}`)
          return []
        }
      })

      // å¹¶è¡Œæ‰¹å¤„ç†æ‰§è¡Œ
      console.log(`\nâš¡ === PARALLEL BATCH PROCESSING ===`)
      const batchResults = await this.batchProcessor.processBatches(
        searchFunctions,
        (completed, total) => {
          const percentage = ((completed / total) * 100).toFixed(1)
          console.log(`ğŸ“Š Progress: ${completed}/${total} tasks completed (${percentage}%)`)
        }
      )

      // æ”¶é›†æ‰€æœ‰ç»“æœ
      for (const posts of batchResults) {
        if (Array.isArray(posts)) {
          allPosts.push(...posts)
        }
      }

      console.log(`âœ… Parallel Reddit search completed: ${allPosts.length} unique posts collected`)

    } catch (error) {
      console.error('âŒ Parallel Reddit search failed:', error.message)
    }

    // æœ€ç»ˆæ’åºå’Œé™åˆ¶
    const sortedPosts = allPosts.sort((a, b) => {
      // ä¼˜å…ˆè€ƒè™‘åˆ†æ•°
      if (b.score !== a.score) return b.score - a.score
      // ç„¶åè€ƒè™‘è¯„è®ºæ•°é‡
      return (b.commentCount || 0) - (a.commentCount || 0)
    })

    const finalPosts = maxPosts ? sortedPosts.slice(0, maxPosts) : sortedPosts
    
    console.log(`\nğŸ¯ === OPTIMIZED REDDIT SCRAPING COMPLETED ===`)
    console.log(`ğŸ“Š Total unique posts: ${allPosts.length}`)
    console.log(`âœ¨ Final posts (after limit): ${finalPosts.length}`)
    console.log(`âš¡ Parallel processing used`)
    console.log(`ğŸ¯ Task-based search strategy`)
    console.log(`ğŸ”„ Real-time deduplication`)
    console.log(`â° End Time: ${new Date().toISOString()}`)
    
    return finalPosts
  }
}

// ä¸»å¤„ç†å‡½æ•°
Deno.serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders })
  }

  try {
    const { 
      appName, 
      userSearchTerm, // ğŸ†• æ¥æ”¶ç”¨æˆ·åŸå§‹æœç´¢è¯
      scrapingSessionId, 
      maxPosts // ç§»é™¤é»˜è®¤çš„400é™åˆ¶ 
    }: ScrapeRequest = await req.json()

    // è‡³å°‘éœ€è¦ä¸€ä¸ªæœç´¢æ¡ä»¶
    if (!appName && !userSearchTerm) {
      return new Response(
        JSON.stringify({ error: 'Missing appName or userSearchTerm parameter' }),
        { 
          status: 400, 
          headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
        }
      )
    }

    console.log(`ğŸš€ Optimized Reddit scraping started`)
    console.log(`ğŸ‘¤ User search term: "${userSearchTerm || 'not provided'}"`)
    console.log(`ğŸ“± App name: "${appName || 'not provided'}"`)
    console.log(`ğŸ”‘ Reddit API status: ${REDDIT_CLIENT_ID ? 'Configured' : 'Not configured'}`)
    console.log(`ğŸ“Š Target max posts: ${maxPosts || 'unlimited - scraping all posts'}`)

    const scraper = new OptimizedRedditScraper()
    const posts = await scraper.scrapeReddit(userSearchTerm, appName, maxPosts)

    // ä¿å­˜åˆ°æ•°æ®åº“å¹¶æ›´æ–°scraperçŠ¶æ€
    if (scrapingSessionId) {
      try {
        console.log(`ğŸ’¾ Saving ${posts.length} posts to database...`)
        
        const supabaseClient = createClient(
          Deno.env.get('SUPABASE_URL') ?? '',
          Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
        )

        // æ›´æ–°scraperçŠ¶æ€ä¸ºrunning
        await supabaseClient
          .from('scraping_sessions')
          .update({
            reddit_scraper_status: 'running',
            reddit_started_at: new Date().toISOString()
          })
          .eq('id', scrapingSessionId)

        const postsToSave = posts.map(post => ({
          scraping_session_id: scrapingSessionId,
          platform: 'reddit' as const,
          review_text: post.text,
          rating: null,
          review_date: post.date,
          author_name: post.author,
          source_url: post.url,
          additional_data: {
            title: post.title,
            score: post.score,
            subreddit: post.subreddit,
            search_term: post.searchTerm,
            upvote_ratio: post.upvoteRatio,
            comment_count: post.commentCount,
            post_id: post.postId,
            gilded: post.gilded,
            is_stickied: post.isStickied,
            scraper_version: 'enhanced_api_v7.0',
            user_search_term: userSearchTerm, // ğŸ†• è®°å½•ç”¨æˆ·æœç´¢è¯
            app_name_used: appName, // ğŸ†• è®°å½•åº”ç”¨å
            search_strategy: 'user_term_priority_enhanced'
          }
        }))

        // åˆ†æ‰¹ä¿å­˜
        const batchSize = 50
        for (let i = 0; i < postsToSave.length; i += batchSize) {
          const batch = postsToSave.slice(i, i + batchSize)
          
          const { error: saveError } = await supabaseClient
            .from('scraped_reviews')
            .insert(batch)

          if (saveError) {
            console.error(`âŒ Database save error for batch ${Math.floor(i/batchSize) + 1}:`, saveError)
          } else {
            console.log(`âœ… Saved batch ${Math.floor(i/batchSize) + 1}: ${batch.length} posts`)
          }
        }

        console.log(`âœ… Successfully saved all ${postsToSave.length} Reddit posts to database`)
        console.log(`ğŸ“Š === REDDIT SCRAPING & SAVING SUMMARY ===`)
        console.log(`ğŸ” Total posts scraped from Reddit API: ${posts.length}`)
        console.log(`ğŸ’¾ Total posts saved to database: ${postsToSave.length}`)
        console.log(`ğŸ“ˆ Save success rate: ${postsToSave.length > 0 ? '100%' : '0%'}`)
        console.log(`ğŸ¯ User search term: "${userSearchTerm || 'not provided'}"`)
        console.log(`ğŸ“± App name used: "${appName || 'not provided'}"`)
        console.log(`â° Scraping completed at: ${new Date().toISOString()}`)

        // ğŸ†• æŸ¥è¯¢å®é™…ä¿å­˜åˆ°æ•°æ®åº“çš„redditæ•°é‡
        const { count: actualSavedCount, error: countError } = await supabaseClient
          .from('scraped_reviews')
          .select('*', { count: 'exact', head: true })
          .eq('scraping_session_id', scrapingSessionId)
          .eq('platform', 'reddit');

        const finalRedditCount = actualSavedCount || 0;
        console.log(`ğŸ“Š Redditå®é™…ä¿å­˜æ•°é‡: ${finalRedditCount} (åŸè®¡åˆ’: ${posts.length})`);

        // æ›´æ–°scraperçŠ¶æ€ä¸ºcompletedï¼ˆåˆ é™¤reviewæ•°é‡å­—æ®µï¼‰
        await supabaseClient
          .from('scraping_sessions')
          .update({
            reddit_scraper_status: 'completed',
            reddit_completed_at: new Date().toISOString()
          })
          .eq('id', scrapingSessionId)

        console.log(`âœ… Reddit scraper status updated to completed`)

      } catch (saveError) {
        console.error('âŒ Error saving Reddit posts to database:', saveError)

        // æ›´æ–°scraperçŠ¶æ€ä¸ºfailed
        try {
          const supabaseClient = createClient(
            Deno.env.get('SUPABASE_URL') ?? '',
            Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
          )
          
          await supabaseClient
            .from('scraping_sessions')
            .update({
              reddit_scraper_status: 'failed',
              reddit_completed_at: new Date().toISOString(),
              reddit_error_message: saveError.message
            })
            .eq('id', scrapingSessionId)
        } catch (updateError) {
          console.error('âŒ Failed to update scraper status:', updateError)
        }
      }
    }

    // è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    const stats = {
      totalPosts: posts.length,
      targetMaxPosts: maxPosts || 'unlimited',
      subreddits: [...new Set(posts.map(p => p.subreddit))],
      averageScore: posts.length > 0 ? Math.round(posts.reduce((sum, p) => sum + p.score, 0) / posts.length) : 0,
      dateRange: posts.length > 0 ? {
        earliest: Math.min(...posts.map(p => new Date(p.date).getTime())),
        latest: Math.max(...posts.map(p => new Date(p.date).getTime()))
      } : null,
      searchStrategy: {
        userSearchTerm: userSearchTerm || null,
        appNameUsed: appName || null,
        strategy: 'user_term_priority_enhanced_api'
      },
      apiUsed: REDDIT_CLIENT_ID ? true : false,
      gildedPosts: posts.filter(p => (p.gilded || 0) > 0).length
    }

    console.log(`\nğŸ“Š === OPTIMIZED REDDIT SCRAPING STATISTICS ===`)
    console.log(`âœ… Total posts scraped: ${stats.totalPosts}`)
    console.log(`ğŸ¯ Target was: ${stats.targetMaxPosts}`)
    console.log(`ğŸ“ˆ Achievement rate: ${typeof stats.targetMaxPosts === 'number' ? ((stats.totalPosts / stats.targetMaxPosts) * 100).toFixed(1) + '%' : 'unlimited mode'}`)
    console.log(`ğŸ“ˆ Average Reddit score: ${stats.averageScore}`)
    console.log(`ğŸ·ï¸ Subreddits found: ${stats.subreddits.length}`)
    console.log(`ğŸ”‘ Reddit API used: ${stats.apiUsed}`)
    console.log(`ğŸ† Gilded posts: ${stats.gildedPosts}`)
    console.log(`ğŸ’¾ Posts that will be saved to database: ${stats.totalPosts}`)
    console.log(`âš¡ Performance: Optimized parallel processing used`)

    return new Response(
      JSON.stringify({ 
        posts,
        stats,
        message: `ğŸš€ OPTIMIZED Reddit scraping completed: ${posts.length} posts scraped and saved to database using parallel processing with user search term "${userSearchTerm || 'not provided'}" and app keywords from "${appName || 'not provided'}"`,
        timestamp: new Date().toISOString(),
        scraper_version: 'enhanced_api_v7.0',
        search_optimization: {
          user_term_priority: true,
          app_name_keywords: true,
          enhanced_api_strategy: true,
          app_specific_subreddits: true,
          advanced_search_patterns: true
        }
      }),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    )

  } catch (error) {
    console.error('âŒ Critical error in Optimized Reddit scraping:', error)
    return new Response(
      JSON.stringify({ 
        error: 'Failed to scrape Reddit',
        details: error.message,
        posts: [],
        stats: {
          totalPosts: 0,
          errorCount: 1,
          scraper_version: 'enhanced_api_v7.0'
        },
        timestamp: new Date().toISOString()
      }),
      { 
        status: 500, 
        headers: { ...corsHeaders, 'Content-Type': 'application/json' } 
      }
    )
  }
})